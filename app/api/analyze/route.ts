// app/api/analyze/route.ts
import { NextResponse } from "next/server";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { tavily } from "@tavily/core";
import { StartupMCTS, type Stats } from "@/lib/mcts";
import { JsonOutputParser } from "@langchain/core/output_parsers";
import { PromptTemplate } from "@langchain/core/prompts";

// âœ… Vercel ìºì‹œ/ISR ì˜í–¥ ì œê±°
export const dynamic = "force-dynamic";
export const maxDuration = 60;
// âœ… (ê¶Œì¥) Edge ëŸ°íƒ€ì„ ë°©ì§€
export const runtime = "nodejs";

// ------------------------------
// 1) Gemini ëª¨ë¸ ëª©ë¡(ListModels) ê¸°ë°˜ Fallback
// ------------------------------
type ModelInfo = {
  name?: string;
  supportedGenerationMethods?: string[];
};

const MODEL_CACHE_TTL_MS = 10 * 60 * 1000;

declare global {
  // eslint-disable-next-line no-var
  var __GEMINI_MODEL_CACHE__: { ts: number; models: string[] } | undefined;
}

function extractErrMsg(e: any): string {
  const parts: string[] = [];
  if (e?.message) parts.push(String(e.message));
  if (e?.cause?.message && e.cause.message !== e.message) parts.push(`cause: ${e.cause.message}`);
  return parts.join(" | ") || String(e);
}

async function fetchAvailableModels(apiKey: string): Promise<string[]> {
  const cache = globalThis.__GEMINI_MODEL_CACHE__;
  if (cache && Date.now() - cache.ts < MODEL_CACHE_TTL_MS && cache.models.length) return cache.models;

  const url = `https://generativelanguage.googleapis.com/v1beta/models?key=${encodeURIComponent(apiKey)}`;

  const res = await fetch(url, {
    method: "GET",
    headers: { "Content-Type": "application/json" },
    cache: "no-store",
  });

  if (!res.ok) {
    throw new Error(`ListModels ì‹¤íŒ¨: HTTP ${res.status} ${res.statusText}`);
  }

  const data = (await res.json()) as { models?: ModelInfo[] };

  const models =
    (data.models ?? [])
      .filter((m) => (m.supportedGenerationMethods ?? []).includes("generateContent"))
      .map((m) => (m.name ?? "").replace(/^models\//, ""))
      .filter(Boolean) ?? [];

  globalThis.__GEMINI_MODEL_CACHE__ = { ts: Date.now(), models };
  return models;
}

function buildFallbackModels(available: string[]): string[] {
  const preferredPatterns = [
    "gemini-2.0-flash",
    "gemini-2.0-flash-exp",
    "gemini-1.5-flash",
    "gemini-1.5-pro",
    "gemini-1.0-pro",
    "gemini-pro",
  ];

  const availSet = new Set(available);
  const picked: string[] = [];

  const pickOne = (pattern: string): string | null => {
    if (availSet.has(pattern)) return pattern;
    const starts = available.find((m) => m.startsWith(pattern));
    return starts ?? null;
  };

  for (const p of preferredPatterns) {
    const m = pickOne(p);
    if (m && !picked.includes(m)) picked.push(m);
  }

  return picked.length ? picked : available.slice(0, 3);
}

function hardFallbackModels(): string[] {
  return ["gemini-2.0-flash-exp", "gemini-1.5-flash", "gemini-1.5-pro", "gemini-1.0-pro", "gemini-pro"];
}

// ------------------------------
// 2) ëª¨ë¸ í˜¸ì¶œ ìœ í‹¸ (JSON / TEXT ë¶„ë¦¬)
// ------------------------------
async function getModelCandidates(apiKey: string): Promise<string[]> {
  try {
    const available = await fetchAvailableModels(apiKey);
    return buildFallbackModels(available);
  } catch (e) {
    console.warn(`âš ï¸ ListModels ì‹¤íŒ¨ -> í•˜ë“œì½”ë”© ëª¨ë¸ë¡œ fallback: ${extractErrMsg(e)}`);
    return hardFallbackModels();
  }
}

// âœ… ì—¬ê¸° ì œë„¤ë¦­ constraintë¥¼ Record<string, any> -> object ë¡œ ì™„í™” (TS ì—ëŸ¬ ë°©ì§€)
async function generateJsonWithFallback<T extends object>(
  apiKey: string,
  prompt: PromptTemplate,
  inputVariables: Record<string, any>,
  parser: JsonOutputParser<T>,
  temperature = 0.35
): Promise<T> {
  const models = await getModelCandidates(apiKey);
  let lastError: any = null;

  for (const modelName of models) {
    try {
      const llm = new ChatGoogleGenerativeAI({ model: modelName, apiKey, temperature });
      const chain = prompt.pipe(llm).pipe(parser);
      return (await chain.invoke(inputVariables)) as T;
    } catch (e: any) {
      console.warn(`âš ï¸ ëª¨ë¸ ì‹¤íŒ¨(JSON): ${modelName} -> ${extractErrMsg(e)}`);
      lastError = e;
    }
  }

  throw new Error(`ëª¨ë“  Gemini ëª¨ë¸(JSON) í˜¸ì¶œ ì‹¤íŒ¨. last=${extractErrMsg(lastError)}`);
}

async function generateTextWithFallback(
  apiKey: string,
  prompt: PromptTemplate,
  inputVariables: Record<string, any>,
  temperature = 0.35
): Promise<string> {
  const models = await getModelCandidates(apiKey);
  let lastError: any = null;

  for (const modelName of models) {
    try {
      const llm = new ChatGoogleGenerativeAI({ model: modelName, apiKey, temperature });
      const chain = prompt.pipe(llm);
      const res: any = await chain.invoke(inputVariables);

      if (typeof res === "string") return res;
      if (res?.content != null) return String(res.content);
      return JSON.stringify(res);
    } catch (e: any) {
      console.warn(`âš ï¸ ëª¨ë¸ ì‹¤íŒ¨(TEXT): ${modelName} -> ${extractErrMsg(e)}`);
      lastError = e;
    }
  }

  throw new Error(`ëª¨ë“  Gemini ëª¨ë¸(TEXT) í˜¸ì¶œ ì‹¤íŒ¨. last=${extractErrMsg(lastError)}`);
}

function toInt0to100(v: any): number {
  const n = Number(v);
  if (!Number.isFinite(n)) return 0;
  return Math.max(0, Math.min(100, Math.round(n)));
}

// âœ… ë§ˆí¬ë‹¤ìš´ ì°Œêº¼ê¸° ì œê±°(íŠ¹íˆ ** ë•Œë¬¸ì— ì§œì¦ë‚˜ëŠ” ì¼€ì´ìŠ¤ ë°©ì§€)
function stripMarkdownArtifacts(s: any): string {
  const text = String(s ?? "");
  return text
    .replace(/\*\*/g, "")          // ** ì œê±°
    .replace(/`+/g, "")            // ë°±í‹± ì œê±°
    .replace(/^\s*[-*]\s+/gm, "")  // - bullet ì œê±°
    .replace(/^#+\s*/gm, "")       // # heading ì œê±°
    .trim();
}

// ------------------------------
// 3) API í•¸ë“¤ëŸ¬
// ------------------------------
export async function POST(req: Request) {
  try {
    const tavilyKey = process.env.TAVILY_API_KEY;
    const googleKey = process.env.GOOGLE_API_KEY;

    if (!tavilyKey || !googleKey) {
      return NextResponse.json(
        { success: false, error: "API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. Vercel í™˜ê²½ë³€ìˆ˜(Settings)ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”." },
        { status: 500 }
      );
    }

    const body = await req.json().catch(() => null);
    const language = body?.language ?? "ko";
    const sellerInfo = body?.sellerInfo ?? "";
    const buyerInfo = body?.buyerInfo ?? "";
    const productInfo = body?.productInfo ?? null;
    const founderTraits = body?.founderTraits ?? null;

    if (!productInfo?.name || !productInfo?.desc) {
      return NextResponse.json(
        { success: false, error: "í•„ìˆ˜ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. (productInfo.name, productInfo.desc)" },
        { status: 400 }
      );
    }

    console.log("ğŸ”¥ ë¶„ì„ ì‹œì‘:", productInfo.name);

    // --- Tavily ê²€ìƒ‰ ---
    const tvly = tavily({ apiKey: tavilyKey });
    let marketData = "ì‹œì¥ ë°ì´í„° ì—†ìŒ";
    let pastCases: Array<{ title: string; url: string; content: string }> = [];

    try {
      const q = `${productInfo.name} ì‹¤íŒ¨ ì‚¬ë¡€ ê²½ìŸì‚¬ ë¦¬ë·° ë¶ˆë§Œ ëŒ€ì²´ì¬`;
      const searchResult = await tvly.search(q, { searchDepth: "advanced", maxResults: 5 });

      marketData = (searchResult.results ?? [])
        .map((r: any) => `- ${r.title}: ${String(r.content).slice(0, 400)}...`)
        .join("\n");

      pastCases = (searchResult.results ?? []).map((r: any) => ({
        title: r.title,
        url: r.url,
        content: r.content,
      }));
    } catch (e: any) {
      console.error("Tavily ê²€ìƒ‰ ì‹¤íŒ¨(ë¬´ì‹œí•˜ê³  ì§„í–‰):", extractErrMsg(e));
    }

    // --- Stats JSON ---
    // âœ… Stats íƒ€ì…ì´ ì´ì œ { product, founder, strategy, marketing, consumer_needs } ì„ (mcts.ts ê¸°ì¤€)
    const statsParser = new JsonOutputParser<Stats>();

    const statsPrompt = PromptTemplate.fromTemplate(
      `ë„ˆëŠ” ëƒ‰ì†Œì ì¸ ìŠ¤íƒ€íŠ¸ì—… ê²€ì¦ê´€ì´ë‹¤.
ì•„ë˜ ì •ë³´ì™€ ì‹œì¥ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 5ëŒ€ ìŠ¤íƒ¯(0~100 ì •ìˆ˜)ì„ JSONìœ¼ë¡œ ì¶œë ¥í•˜ë¼.

ì¤‘ìš”:
- ì´ˆê¸° ìŠ¤íƒ€íŠ¸ì—…ì€ íŒ€ì´ ì—†ì„ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ 'team'ì„ í‰ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤.
- ëŒ€ì‹  ì°½ì—…ì ê°œì¸ ì—­ëŸ‰ì„ 'founder' ì ìˆ˜ë¡œ í‰ê°€í•œë‹¤.
- founder ì ìˆ˜ëŠ” ì•„ë˜ 'ì°½ì—…ì íŠ¹ì„±(1~10)'ì„ ê°•í•˜ê²Œ ë°˜ì˜í•˜ë¼.
- strategy ì ìˆ˜ì—ë„ ì°½ì—…ì íŠ¹ì„±(ì‹¤í–‰ë ¥/ë¶ˆí™•ì‹¤ì„± ë‚´ì„±/ì„¤ë“ë ¥/ë¦¬ì†ŒìŠ¤ ê°ê°)ì„ ë°˜ì˜í•˜ë¼.

ì…ë ¥ ì •ë³´:
- íŒë§¤ì: {sellerInfo}
- íƒ€ê²Ÿ: {buyerInfo}
- ì•„ì´í…œ: {productInfo}
- ì°½ì—…ì íŠ¹ì„±(1~10): {founderTraits}

ì‹œì¥ ë°ì´í„°:
{marketData}

ì£¼ì˜:
- JSONë§Œ ì¶œë ¥ (ì„¤ëª…/ë¬¸ì¥ ê¸ˆì§€)
- ê°’ì€ 0~100 ì •ìˆ˜

{format_instructions}
JSON í‚¤: product, founder, strategy, marketing, consumer_needs`
    );

    const rawStats = await generateJsonWithFallback<Stats>(
      googleKey,
      statsPrompt,
      {
        sellerInfo,
        buyerInfo,
        productInfo: JSON.stringify(productInfo),
        founderTraits: JSON.stringify(founderTraits ?? {}),
        marketData,
        format_instructions: statsParser.getFormatInstructions(),
      },
      statsParser,
      0.3
    );

    const safeStats: Stats = {
      product: toInt0to100((rawStats as any).product),
      founder: toInt0to100((rawStats as any).founder),
      strategy: toInt0to100((rawStats as any).strategy),
      marketing: toInt0to100((rawStats as any).marketing),
      consumer_needs: toInt0to100((rawStats as any).consumer_needs),
    };

    // --- MCTS ---
    const mcts = new StartupMCTS(1500);
    const simulation = mcts.run(safeStats);

    // --- Report JSON (ìœ íŠœë¸Œ ì¶”ì²œ ì¿¼ë¦¬ + í‚¤ì›Œë“œ í¬í•¨) ---
    type ReportShape = {
      death_cause: string;
      autopsy_report: string;
      action_plan: string;
      needs_analysis: string;
      youtube_queries: string[];
      keywords: string[];
    };

    const reportParser = new JsonOutputParser<ReportShape>();
    const reportPrompt = PromptTemplate.fromTemplate(
      `ë„ˆëŠ” ëƒ‰ì†Œì ì¸ VCë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ë¶€ê²€ ë¦¬í¬íŠ¸'ë¥¼ JSONìœ¼ë¡œ ì‘ì„±í•˜ë¼.

ìš”êµ¬ JSON í‚¤:
- death_cause (ì§§ê²Œ)
- autopsy_report (ì¤„ê¸€)
- needs_analysis (ì¤„ê¸€)
- action_plan (ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ë¥¼ "1) ...\\n2) ..." í˜•íƒœë¡œ. ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€. **, *, # ê°™ì€ ê¸°í˜¸ ì“°ì§€ ë§ˆ.)
- youtube_queries (ë°°ì—´, string 3ê°œ: "ì•„ì´í…œ/ì‹œì¥/ì‹¤íŒ¨ì‚¬ë¡€"ë¡œ ìœ íŠœë¸Œ ê²€ìƒ‰í•  ë¬¸ì¥)
- keywords (ë°°ì—´, string 10ê°œ: ì›Œë“œí´ë¼ìš°ë“œìš© í•µì‹¬ í‚¤ì›Œë“œ)

ì…ë ¥:
- ìŠ¤íƒ¯: {stats}
- ê°€ì¥ ë§ì´ ì£½ì€ êµ¬ê°„: {bottleneck}
- ì‹œì¥ë°ì´í„°: {marketData}

ì£¼ì˜:
- JSONë§Œ ì¶œë ¥
- action_planì— ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€(íŠ¹íˆ ** ì‚¬ìš© ê¸ˆì§€)
- keywordsëŠ” "ë‹¨ì–´/ì§§ì€ êµ¬" ì¤‘ì‹¬

{format_instructions}`
    );

    const reportRaw = await generateJsonWithFallback<ReportShape>(
      googleKey,
      reportPrompt,
      {
        stats: JSON.stringify(safeStats),
        bottleneck: (simulation as any).bottleneck_stage ?? (simulation as any).bottleneck ?? "",
        marketData,
        format_instructions: reportParser.getFormatInstructions(),
      },
      reportParser,
      0.35
    );

    // âœ… action_planì—ì„œ ë§ˆí¬ë‹¤ìš´ ì°Œêº¼ê¸° 2ì°¨ ì œê±°
    const report: ReportShape = {
      ...reportRaw,
      action_plan: stripMarkdownArtifacts(reportRaw.action_plan),
      autopsy_report: String(reportRaw.autopsy_report ?? ""),
      needs_analysis: String(reportRaw.needs_analysis ?? ""),
      death_cause: String(reportRaw.death_cause ?? ""),
      youtube_queries: Array.isArray(reportRaw.youtube_queries) ? reportRaw.youtube_queries.slice(0, 3) : [],
      keywords: Array.isArray(reportRaw.keywords) ? reportRaw.keywords.slice(0, 10) : [],
    };

    // --- Debate TEXT ---
    const debateLangInstr =
      language === "en"
        ? "Write the conversation in natural English."
        : "í•œêµ­ì–´ ëŒ€í™”ì²´ë¡œ ì‘ì„±.";

    const debatePrompt = PromptTemplate.fromTemplate(
      `ì•„ë˜ ì •ë³´ë¥¼ ë³´ê³  3ëª…ì˜ ì „ë¬¸ê°€ê°€ ë…ì„¤ ì¢Œë‹´íšŒë¥¼ ì—´ì–´ë¼.
${debateLangInstr}

1) ë§ˆí¬êµ¬ VC (ëƒ‰ì†Œì ) 2) í…Œí—¤ë€ë¡œ ì°½ì—…ê°€ (í˜„ì‹¤ì ) 3) ê¹Œì¹ í•œ ì–¼ë¦¬ì–´ë‹µí„° (ë¶ˆë§Œ ë§ìŒ)

ì•„ì´í…œ: {item}
ìŠ¤íƒ¯: {stats}
ì‹œì¥ë°ì´í„° ìš”ì•½: {marketData}

í˜•ì‹:
- ëŒ€í™”ì²´ë¡œ ì¤„ë°”ê¿ˆ
- ë§ˆì§€ë§‰ ì¤„ì— "ê²°ë¡ : í•œ ì¤„"ë¡œ ëë‚´ë¼`
    );

    const debate = await generateTextWithFallback(
      googleKey,
      debatePrompt,
      {
        item: JSON.stringify(productInfo),
        stats: JSON.stringify(safeStats),
        marketData,
      },
      0.45
    );

    return NextResponse.json({
      success: true,
      stats: safeStats,     // âœ… founder í¬í•¨
      simulation,           // âœ… survival_rate / death_counts / bottleneck_stage í¬í•¨
      report,               // âœ… youtube_queries + keywords ìœ ì§€
      debate,
      pastCases,            // âœ… ìœ ì‚¬/ì‹¤íŒ¨ì‚¬ë¡€ ë§í¬ ìœ ì§€
    });
  } catch (error: any) {
    console.error("Server Error:", extractErrMsg(error));
    return NextResponse.json({ success: false, error: extractErrMsg(error) }, { status: 500 });
  }
}
